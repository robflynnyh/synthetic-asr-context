import argparse
from tqdm import tqdm
from synctxasr.data import dataset_functions
from synctxasr.wer import word_error_rate_detail
from synctxasr.misc import int_or_none
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers import DataCollatorWithPadding
from transformers import Trainer, TrainingArguments, EarlyStoppingCallback
from trl import GRPOConfig, GRPOTrainer
from accelerate import Accelerator
from peft import LoraConfig
import whisper
from datasets import Dataset
import torch
import os
from functools import partial
import wandb
from synctxasr.wer import word_error_rate_detail    



class CustomDataset(torch.utils.data.Dataset):
    def __init__(self, tokenizer, file_paths):
        self.tokenizer = tokenizer
        self.file_paths = file_paths

    def __len__(self):
        return len(self.file_paths)

    @staticmethod
    def format_example(example):
        # prompt_messages = [
        #     {"role": "system", "content": [{"type": "text", "text": "You are an helpful assistant. The user will supply you with a sentence that was generated by an ASR model. Your task is to predict a plausible sentence that could have come before it. Do not include any other information in your response."}]},
        #     {"role": "user", "content": [{"type": "text", "text": f"'{example}'."}]},
        # ] 
        prompt_messages = [
            {"role": "system", "content": "You are a helpful assistant. The user will supply you with a sentence that was generated by an ASR model. Your task is to predict a plausible sentence that could have come before it. Do not include any other information in your response."},
            {"role": "user", "content": f"'{example}'."},
        ]
        return prompt_messages

    def __getitem__(self, idx):
        data = torch.load(self.file_paths[idx])
        audio = data['waveform'].numpy()
        whisper_generation = data['generation'] 
        gold_text = data['text']
        example = self.format_example(whisper_generation)
        original_wer = word_error_rate_detail([whisper_generation], [gold_text], use_cer=False, normalize=True)[0]

        return {
            'prompt': example,
            'answer': gold_text,
            'audio': audio,
            'original_wer': original_wer,
        }
        
# Dummy reward function: count the number of unique characters in the completions
def reward_func(asr_model):
    def reward_num_unique_chars(prompts, answer, completions, audio, original_wer, **kwargs):
        text_completions = [el[0]['content'] for el in completions]
        results = []
        for i, sample in enumerate(audio):
            result = asr_model.transcribe(
                audio=sample[0], 
                initial_prompt=text_completions[i], 
                without_timestamps=True, 
                language='en', 
                task='transcribe',
                beam_size=1,
            ) 
            print(text_completions[i], text_completions[i].__class__)
            print(result['text'])
            results.append(result['text'].strip())
            print('--')
        # print([el.shape for el in audio])
        print(answer)
        
        wers = [word_error_rate_detail([r], [a], use_cer=False, normalize = True)[0] for r, a in zip(results, answer)]
        improvements = [original_wer[i] - wers[i] for i in range(len(wers))]
        print(improvements)
        #print(completions[0][0])
        return improvements
    return reward_num_unique_chars

def get_dataset(args, tokenizer, split='train', max_files=-1): # preload all text
    path = os.path.join(args.dataset_path, split)
    files = os.listdir(path)
    dataset = []
    for i, file_path in enumerate(tqdm(files)):
        if max_files > 0 and i > max_files: break # for debugging purposes
        full_file_path = os.path.join(path, file_path)
        dataset.append(full_file_path)

    dataset = CustomDataset(tokenizer, dataset)
    return dataset

def main(args):
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    accelerator = Accelerator()
    
    tokenizer = AutoTokenizer.from_pretrained(args.language_model)
    tokenizer.pad_token = tokenizer.eos_token

    train_dataset = get_dataset(args, tokenizer, split='train')
    dev_dataset = get_dataset(args, tokenizer, split='dev')

    model = AutoModelForCausalLM.from_pretrained(args.language_model, torch_dtype=torch.bfloat16).to("cuda")

    asr_model = whisper.load_model(args.whisper_model, device=args.whisper_device)
 
    if not args.no_wandb and accelerator.is_local_main_process: wandb.init(project="synctxasr")


    training_args = GRPOConfig(
        output_dir=args.output_dir,
        learning_rate=args.lr,
        logging_dir=os.path.join(args.output_dir, "logs"),
        logging_steps=1,
        bf16=True,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=args.batch_size,
        num_generations=16,
        warmup_steps=100,
        max_prompt_length=1024,
        max_completion_length=64,
        gradient_checkpointing=False,
        num_train_epochs=args.epochs,
        #eval_strategy="steps",
        save_steps=1000,
        #eval_steps=1000,
        max_grad_norm=0.1,
        report_to=["wandb"] if not args.no_wandb else None,
        log_on_each_node=False,
        dataloader_num_workers=4,
        dataloader_pin_memory=False,
        dataloader_prefetch_factor=4,
        optim="adafactor",
        use_vllm=True,
        beta=0.0,
    )

    # peft_config = LoraConfig(
    #     r=16,
    #     lora_alpha=64,
    #     target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "up_proj", "down_proj", "gate_proj"],
    #     task_type="CAUSAL_LM",
    #     lora_dropout=0.05,
    # )


    trainer = GRPOTrainer(
        model=model,
        processing_class=tokenizer,
        reward_funcs=[
            reward_func(asr_model),
        ],
        args=training_args,
        train_dataset=train_dataset,
        #eval_dataset=dev_dataset,
        #peft_config=peft_config
    )

    #trainer.evaluate()
    trainer.train()
    trainer.save_model(os.path.join(args.output_dir, "final_model"))

    if not args.no_wandb and accelerator.is_local_main_process:
        wandb.finish()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--language_model', type=str, default='/store/store4/data/huggingface_models/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775')
    parser.add_argument('--device', type=str, default='cuda')
    parser.add_argument('--dataset_path', type=str, default='/store/store4/data/TEDLIUM3_Whisper_tiny_en_outputs/')
    parser.add_argument('--no_wandb', action='store_true', help='Disable wandb logging')
    parser.add_argument('--batch_size', type=int, default=32)
    parser.add_argument('--lr', type=float, default=1e-6)
    parser.add_argument('--output_dir', type=str, default='/store/store5/data/acp21rjf_checkpoints/synctxasr/grpo/b0')
    parser.add_argument('--epochs', type=int, default=1)
    parser.add_argument('--gradient_accumulation_steps', type=int, default=1)
    parser.add_argument('--local-rank', type=int, default=0)
    parser.add_argument('--whisper_model', type=str, default='tiny.en')
    parser.add_argument('--whisper-device', type=str, default='cuda:1')

    args = parser.parse_args()    
    if not torch.cuda.is_available(): args.device = 'cpu'

    main(args)