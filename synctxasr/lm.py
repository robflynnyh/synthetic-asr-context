from transformers import AutoTokenizer, AutoModelForCausalLM
import torch 

# def create_lm_template(transcription:str):
#     messages = [
#         {
#             "role": "system",
#             "content": [{"type": "text", "text": "You are a helpful assistant. You do not use full stops at the end of your outputs unless absolutely necessary!"}]
#         },
#         {
#             "role": "user",
#             "content": [{
#                 "type": "text", "text": f"Given the following output from my ASR model, predict a plausable sentence that could have come before it. Only ouptut one sentence and nothing else: '{transcription}'."}]
#         },
#     ]    
#     return messages

def create_lm_template(transcription:str):
    # messages = [
    #     {
    #         "role": "system",
    #         "content": [{"type": "text", "text": "You are a helpful assistant."}]
    #     },
    #     {
    #         "role": "user",
    #         "content": [{
    #             "type": "text", "text": f"Given the following output from the ASR model, please predict a plausable sentence that could have come before it: '{transcription}'."}]
    #     },
    # ]    

    messages = [
        {"role": "system", "content": "You are a helpful assistant. The user will supply you with a sentence that was generated by an ASR model. Your task is to predict a plausible sentence that could have come before it. Do not include any other information in your response."},
        {"role": "user", "content": f"'{transcription}'."},
    ]
    return messages

def create_inputs(transcription:str, tokenizer:AutoTokenizer, device:str):
    messages = create_lm_template(transcription)

    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    inputs = tokenizer([text], return_tensors="pt").to(device)

    return inputs

def generate_lm_response(transcription:str, model:AutoModelForCausalLM, tokenizer:AutoTokenizer, device:str):
    inputs = create_inputs(transcription, tokenizer, device)
    
    with torch.inference_mode():
        outputs = model.generate(
            **inputs,
            pad_token_id=tokenizer.eos_token_id,
            max_new_tokens=512,
            #temperature=1.5,
        )
        
    outputs = tokenizer.batch_decode(outputs)[0]
    #response = outputs.split("<start_of_turn>model\n")[-1].strip().split("<end_of_turn>")[0].strip()
    #response = outputs.split('<|eot_id|><|start_header_id|>assistant<|end_header_id|>')[-1].strip().split('<|eot_id|>')[0].strip()
    response = outputs.split('<|im_start|>assistant')[-1].strip().split('<|im_end|>')[0].strip()

    return response